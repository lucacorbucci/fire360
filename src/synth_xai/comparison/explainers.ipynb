{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcorbucci/synth_xai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import signal\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from types import FrameType\n",
    "from typing import Any\n",
    "\n",
    "import dill\n",
    "import multiprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from synth_xai.explanations.explanation_utils import (\n",
    "    evaluate_bb,\n",
    "    find_top_closest_rows,\n",
    "    get_test_data,\n",
    "    is_explainer_supported,\n",
    "    label_synthetic_data,\n",
    "    load_bb,\n",
    "    load_synthetic_data,\n",
    "    make_predictions,\n",
    "    prepare_neighbours,\n",
    "    setup_wandb,\n",
    "    transform_input_data,\n",
    ")\n",
    "from loguru import logger\n",
    "from multiprocess import Pool\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    ")\n",
    "\n",
    "from synth_xai.bb_architectures import MultiClassModel, SimpleModel\n",
    "from synth_xai.explanations.explainer_model import ExplainerModel\n",
    "from synth_xai.utils import (\n",
    "    prepare_adult,\n",
    "    prepare_dutch,\n",
    "    prepare_letter,\n",
    "    prepare_shuttle, \n",
    "    prepare_covertype, \n",
    "    prepare_house16,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/lcorbucci/synth_xai/artifacts/house16/explanations/dt_ctgan_150000_2500_50.pkl\", \"rb\") as f:\n",
    "    exp_1 = dill.load(f)\n",
    "\n",
    "with open(\"/home/lcorbucci/synth_xai/artifacts/house16/explanations/dt_ctgan_150000_2500_52.pkl\", \"rb\") as f:\n",
    "    exp_2 = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['(P14p9 = 0.120948) > 0.08191400021314621',\n",
       "  '(P27p4 = 0.011348) <= 0.035534000024199486',\n",
       "  '(H2p2 = 0.071146) > 0.057501498609781265',\n",
       "  '(P1 = 1556) > 1303.5',\n",
       "  '(P11p4 = 0.342545) > 0.07796349748969078',\n",
       "  'Leaf node 89 reached, prediction: 1.0'],\n",
       " 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['(P14p9 = 0.120948) > 0.0898665003478527',\n",
       "  '(H40p4 = 0.8) > 0.4696744978427887',\n",
       "  '(P27p4 = 0.011348) <= 0.046157000586390495',\n",
       "  '(H2p2 = 0.071146) > 0.03668750077486038',\n",
       "  '(H13p1 = 0.121212) > 0.11366499960422516',\n",
       "  '(P11p4 = 0.342545) <= 0.43805699050426483',\n",
       "  '(P27p4 = 0.011348) <= 0.03520500101149082',\n",
       "  '(P16p2 = 0.777305) <= 0.7973635196685791',\n",
       "  '(H2p2 = 0.071146) <= 0.312731996178627',\n",
       "  '(H2p2 = 0.071146) > 0.04505950026214123',\n",
       "  'Leaf node 199 reached, prediction: 1.0'],\n",
       " 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.dataset import TabularDataset\n",
    "\n",
    "# from lore_sa.lorem import LOREM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lcorbucci/synth_xai/src/synth_xai/comparison\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 10:51:12,075 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/mstz/covertype/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "2025-03-12 10:51:12,077 urllib3.connectionpool DEBUG    Resetting dropped connection: s3.amazonaws.com\n",
      "2025-03-12 10:51:12,420 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/mstz/covertype/mstz/covertype.py HTTP/1.1\" 404 0\n",
      "2025-03-12 10:51:12,578 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /parquet?dataset=mstz/covertype HTTP/1.1\" 200 None\n",
      "2025-03-12 10:51:12,741 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=mstz/covertype HTTP/1.1\" 200 None\n",
      "2025-03-12 10:51:12,940 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/mstz/covertype/revision/refs%2Fconvert%2Fparquet HTTP/1.1\" 200 825\n",
      "2025-03-12 10:51:13,071 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/mstz/covertype/revision/741d5a8f6f87753225924a0aea59302ed4c7217a HTTP/1.1\" 200 876\n",
      "2025-03-12 10:51:13,237 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/mstz/covertype/tree/741d5a8f6f87753225924a0aea59302ed4c7217a/covertype%2Ftrain?recursive=False&expand=False HTTP/1.1\" 200 235\n",
      "2025-03-12 10:51:13,584 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/mstz/covertype/revision/741d5a8f6f87753225924a0aea59302ed4c7217a HTTP/1.1\" 200 876\n",
      "2025-03-12 10:51:13,585 filelock     DEBUG    Attempting to acquire lock 139642825093808 on /home/lcorbucci/.cache/huggingface/datasets/_home_lcorbucci_.cache_huggingface_datasets_mstz___covertype_covertype_1.0.0_72ae206f21f416f3e7981183537e9ab071aff006.lock\n",
      "2025-03-12 10:51:13,586 filelock     DEBUG    Lock 139642825093808 acquired on /home/lcorbucci/.cache/huggingface/datasets/_home_lcorbucci_.cache_huggingface_datasets_mstz___covertype_covertype_1.0.0_72ae206f21f416f3e7981183537e9ab071aff006.lock\n",
      "2025-03-12 10:51:13,586 fsspec.local DEBUG    open file: /home/lcorbucci/.cache/huggingface/datasets/mstz___covertype/covertype/1.0.0/72ae206f21f416f3e7981183537e9ab071aff006/dataset_info.json\n",
      "2025-03-12 10:51:13,587 filelock     DEBUG    Attempting to release lock 139642825093808 on /home/lcorbucci/.cache/huggingface/datasets/_home_lcorbucci_.cache_huggingface_datasets_mstz___covertype_covertype_1.0.0_72ae206f21f416f3e7981183537e9ab071aff006.lock\n",
      "2025-03-12 10:51:13,587 filelock     DEBUG    Lock 139642825093808 released on /home/lcorbucci/.cache/huggingface/datasets/_home_lcorbucci_.cache_huggingface_datasets_mstz___covertype_covertype_1.0.0_72ae206f21f416f3e7981183537e9ab071aff006.lock\n",
      "2025-03-12 10:51:13,588 filelock     DEBUG    Attempting to acquire lock 139642830454944 on /home/lcorbucci/.cache/huggingface/datasets/mstz___covertype/covertype/1.0.0/72ae206f21f416f3e7981183537e9ab071aff006_builder.lock\n",
      "2025-03-12 10:51:13,589 filelock     DEBUG    Lock 139642830454944 acquired on /home/lcorbucci/.cache/huggingface/datasets/mstz___covertype/covertype/1.0.0/72ae206f21f416f3e7981183537e9ab071aff006_builder.lock\n",
      "2025-03-12 10:51:13,589 fsspec.local DEBUG    open file: /home/lcorbucci/.cache/huggingface/datasets/mstz___covertype/covertype/1.0.0/72ae206f21f416f3e7981183537e9ab071aff006/dataset_info.json\n",
      "2025-03-12 10:51:13,589 filelock     DEBUG    Attempting to release lock 139642830454944 on /home/lcorbucci/.cache/huggingface/datasets/mstz___covertype/covertype/1.0.0/72ae206f21f416f3e7981183537e9ab071aff006_builder.lock\n",
      "2025-03-12 10:51:13,590 filelock     DEBUG    Lock 139642830454944 released on /home/lcorbucci/.cache/huggingface/datasets/mstz___covertype/covertype/1.0.0/72ae206f21f416f3e7981183537e9ab071aff006_builder.lock\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['class'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovertype\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Get the feature names after one-hot encoding\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     31\u001b[0m bb_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../artifacts/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bb/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_BB.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m bb \u001b[38;5;241m=\u001b[39m load_bb(bb_path)\n",
      "File \u001b[0;32m~/synth_xai/.venv/lib/python3.10/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/synth_xai/.venv/lib/python3.10/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/synth_xai/.venv/lib/python3.10/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/synth_xai/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['class'] not found in axis\""
     ]
    }
   ],
   "source": [
    "class MyModel:\n",
    "    def __init__(self, model: torch.nn.Module, scaler) -> None:\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> torch.Tensor:\n",
    "\n",
    "        x = self.scaler.transform(x)\n",
    "\n",
    "        predictions = []\n",
    "        for sample in x:\n",
    "            sample = torch.Tensor(sample)\n",
    "            predictions.append(self.model(sample).argmax().item())\n",
    "        return np.array(predictions)\n",
    "        # print(x)\n",
    "        # x = torch.Tensor(x)\n",
    "        # print(x)\n",
    "        # return np.array([self.model(sample).argmax().item() for sample in x])\n",
    "    \n",
    "\n",
    "seed = 112\n",
    "current_script_path = Path(\"./\").resolve()\n",
    "print(current_script_path)\n",
    "x_train, _, x_test, _, _, _, train_df, test_data = prepare_covertype(sweep=False, seed=seed)#, current_path=current_script_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiClassModel(\n",
       "  (layer1): Linear(in_features=54, out_features=32, bias=True)\n",
       "  (layer2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "target = \"cover_type\"\n",
    "dataset = \"covertype\"\n",
    "# Get the feature names after one-hot encoding\n",
    "feature_names = list(train_df.drop(columns=[target]).columns)\n",
    "\n",
    "bb_path = f\"../../../artifacts/{dataset}/bb/{dataset}_BB.pth\"\n",
    "bb = load_bb(bb_path)\n",
    "\n",
    "bb.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "train_df_tmp = copy.copy(train_df)\n",
    "train_df_tmp = train_df_tmp.drop(columns=[target])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(train_df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(model=bb, scaler=scaler)\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[target] = [int(x) for x in train_df[target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[target] = train_df[target].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elevation</th>\n",
       "      <th>aspect</th>\n",
       "      <th>slope</th>\n",
       "      <th>horizontal_distance_to_hydrology</th>\n",
       "      <th>vertical_distance_to_hydrology</th>\n",
       "      <th>horizontal_distance_to_roadways</th>\n",
       "      <th>hillshade_9am</th>\n",
       "      <th>hillshade_noon</th>\n",
       "      <th>hillshade_3pm</th>\n",
       "      <th>horizontal_distance_to_fire_points</th>\n",
       "      <th>...</th>\n",
       "      <th>soil_type_id_31</th>\n",
       "      <th>soil_type_id_32</th>\n",
       "      <th>soil_type_id_33</th>\n",
       "      <th>soil_type_id_34</th>\n",
       "      <th>soil_type_id_35</th>\n",
       "      <th>soil_type_id_36</th>\n",
       "      <th>soil_type_id_37</th>\n",
       "      <th>soil_type_id_38</th>\n",
       "      <th>soil_type_id_39</th>\n",
       "      <th>cover_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229430</th>\n",
       "      <td>2878.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>2642.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572404</th>\n",
       "      <td>2499.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>618.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>787.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570202</th>\n",
       "      <td>2566.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>997.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55212</th>\n",
       "      <td>2727.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1348.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2783.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535623</th>\n",
       "      <td>3257.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1198.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248501</th>\n",
       "      <td>3034.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203275</th>\n",
       "      <td>3275.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6242.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180452</th>\n",
       "      <td>3210.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5799.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528868</th>\n",
       "      <td>2933.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2704.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1717.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184258</th>\n",
       "      <td>3086.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3478.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>464809 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        elevation  aspect  slope  horizontal_distance_to_hydrology  \\\n",
       "229430     2878.0   145.0   22.0                              60.0   \n",
       "572404     2499.0    67.0   26.0                             319.0   \n",
       "570202     2566.0    19.0   25.0                             319.0   \n",
       "55212      2727.0    41.0   10.0                             488.0   \n",
       "535623     3257.0   114.0   17.0                              42.0   \n",
       "...           ...     ...    ...                               ...   \n",
       "248501     3034.0   243.0    6.0                             240.0   \n",
       "203275     3275.0    82.0    7.0                             430.0   \n",
       "180452     3210.0    52.0    6.0                             467.0   \n",
       "528868     2933.0   122.0   27.0                              85.0   \n",
       "184258     3086.0    39.0    9.0                             382.0   \n",
       "\n",
       "        vertical_distance_to_hydrology  horizontal_distance_to_roadways  \\\n",
       "229430                            10.0                           1513.0   \n",
       "572404                           155.0                            618.0   \n",
       "570202                           123.0                            964.0   \n",
       "55212                             39.0                           1348.0   \n",
       "535623                            10.0                            721.0   \n",
       "...                                ...                              ...   \n",
       "248501                            49.0                           1250.0   \n",
       "203275                            23.0                           6242.0   \n",
       "180452                            31.0                           5799.0   \n",
       "528868                            32.0                           2704.0   \n",
       "184258                            42.0                           3478.0   \n",
       "\n",
       "        hillshade_9am  hillshade_noon  hillshade_3pm  \\\n",
       "229430          244.0           230.0          104.0   \n",
       "572404          235.0           178.0           59.0   \n",
       "570202          192.0           178.0          112.0   \n",
       "55212           221.0           218.0          131.0   \n",
       "535623          247.0           220.0           96.0   \n",
       "...               ...             ...            ...   \n",
       "248501          207.0           245.0          176.0   \n",
       "203275          230.0           228.0          132.0   \n",
       "180452          223.0           227.0          140.0   \n",
       "528868          253.0           207.0           62.0   \n",
       "184258          221.0           219.0          133.0   \n",
       "\n",
       "        horizontal_distance_to_fire_points  ...  soil_type_id_31  \\\n",
       "229430                              2642.0  ...            False   \n",
       "572404                               787.0  ...            False   \n",
       "570202                               997.0  ...            False   \n",
       "55212                               2783.0  ...            False   \n",
       "535623                              1198.0  ...            False   \n",
       "...                                    ...  ...              ...   \n",
       "248501                              1998.0  ...             True   \n",
       "203275                               603.0  ...            False   \n",
       "180452                              2753.0  ...            False   \n",
       "528868                              1717.0  ...            False   \n",
       "184258                               680.0  ...            False   \n",
       "\n",
       "        soil_type_id_32  soil_type_id_33  soil_type_id_34  soil_type_id_35  \\\n",
       "229430            False            False            False            False   \n",
       "572404            False            False            False            False   \n",
       "570202            False            False            False            False   \n",
       "55212             False            False            False            False   \n",
       "535623            False            False            False            False   \n",
       "...                 ...              ...              ...              ...   \n",
       "248501            False            False            False            False   \n",
       "203275            False            False            False            False   \n",
       "180452            False            False            False            False   \n",
       "528868            False            False            False            False   \n",
       "184258            False            False            False            False   \n",
       "\n",
       "        soil_type_id_36  soil_type_id_37  soil_type_id_38  soil_type_id_39  \\\n",
       "229430            False            False            False            False   \n",
       "572404            False            False            False            False   \n",
       "570202            False            False            False            False   \n",
       "55212             False            False            False            False   \n",
       "535623            False            False            False            False   \n",
       "...                 ...              ...              ...              ...   \n",
       "248501            False            False            False            False   \n",
       "203275            False            False            False            False   \n",
       "180452            False            False            False            False   \n",
       "528868            False            False            False            False   \n",
       "184258            False            False            False            False   \n",
       "\n",
       "        cover_type  \n",
       "229430           1  \n",
       "572404           1  \n",
       "570202           1  \n",
       "55212            1  \n",
       "535623           1  \n",
       "...            ...  \n",
       "248501           1  \n",
       "203275           0  \n",
       "180452           1  \n",
       "528868           0  \n",
       "184258           0  \n",
       "\n",
       "[464809 rows x 55 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TabularDataset.from_dict(train_df, class_name = target)\n",
    "dataset.df.dropna(inplace = True)\n",
    "dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['numeric', 'categorical', 'ordinal', 'target'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.descriptor.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric:  10\n",
      "categorical:  44\n",
      "ordinal:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"numeric: \", len(dataset.descriptor[\"numeric\"]))\n",
    "print(\"categorical: \", len(dataset.descriptor[\"categorical\"]))\n",
    "print(\"ordinal: \", len(dataset.descriptor[\"ordinal\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.bbox import sklearn_classifier_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = sklearn_classifier_bbox.sklearnBBox(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.lore import TabularRandomGeneratorLore\n",
    "\n",
    "tabularLore = TabularRandomGeneratorLore(bbox, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row = 10\n",
    "x = dataset.df.iloc[num_row][:-1] # we exclude the target feature\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 100):\n",
    "#     x = dataset.df.iloc[i][:-1] # we exclude the target feature\n",
    "#     print(bb(torch.Tensor(scaler.transform(np.array(x.values).reshape(1, -1)))).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100\n",
    "multi_x = dataset.df.iloc[:num_row] # we exclude the target feature\n",
    "multi_x = multi_x.drop(columns = [\"income_binary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb(torch.Tensor(scaler.transform(np.array(x.values).reshape(1, -1)))).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(multi_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptor.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptor[\"categorical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptor[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when\n",
    "explanation = tabularLore.explain(x)\n",
    "# then\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for premise in explanation[\"rule\"]['premises']:\n",
    "    print(premise[\"attr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/lcorbucci/synth_xai/artifacts/adult/comparison_explanation/lore/lore_1.pkl\", \"rb\") as f:\n",
    "    shap_explanation = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explanation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explanation[1][\"rule\"][\"premises\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lore - Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.dataset import TabularDataset\n",
    "\n",
    "# from lore_sa.lorem import LOREM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    def __init__(self, model: torch.nn.Module, scaler) -> None:\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> torch.Tensor:\n",
    "\n",
    "        x = self.scaler.transform(x)\n",
    "\n",
    "        predictions = []\n",
    "        for sample in x:\n",
    "            sample = torch.Tensor(sample)\n",
    "            predictions.append(self.model(sample).argmax().item())\n",
    "        return np.array(predictions)\n",
    "        # print(x)\n",
    "        # x = torch.Tensor(x)\n",
    "        # print(x)\n",
    "        # return np.array([self.model(sample).argmax().item() for sample in x])\n",
    "    \n",
    "\n",
    "seed = 112\n",
    "current_script_path = Path(\"./\").resolve()\n",
    "print(current_script_path)\n",
    "x_train, _, x_test, _, _, _, train_df, test_data = prepare_dutch(sweep=False, seed=seed, current_path=current_script_path)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "feature_names = list(train_df.drop(columns=[\"occupation_binary\"]).columns)\n",
    "\n",
    "bb_path = \"../../../artifacts/dutch/bb/dutch_BB.pth\"\n",
    "bb = load_bb(bb_path)\n",
    "\n",
    "bb.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "train_df_tmp = copy.copy(train_df)\n",
    "train_df_tmp = train_df_tmp.drop(columns=[\"occupation_binary\"])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(train_df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(model=bb, scaler=scaler)\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"occupation_binary\"] = [int(x) for x in train_df[\"occupation_binary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"occupation_binary\"] = train_df[\"occupation_binary\"].astype('category')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TabularDataset.from_dict(train_df, class_name = \"occupation_binary\")\n",
    "dataset.df.dropna(inplace = True)\n",
    "dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptor.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptor[\"numeric\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.bbox import sklearn_classifier_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = sklearn_classifier_bbox.sklearnBBox(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.lore import TabularRandomGeneratorLore\n",
    "\n",
    "tabularLore = TabularRandomGeneratorLore(bbox, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row = 10\n",
    "x = dataset.df.iloc[num_row][:-1] # we exclude the target feature\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 100):\n",
    "#     x = dataset.df.iloc[i][:-1] # we exclude the target feature\n",
    "#     print(bb(torch.Tensor(scaler.transform(np.array(x.values).reshape(1, -1)))).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100\n",
    "multi_x = dataset.df.iloc[:num_row] # we exclude the target feature\n",
    "multi_x = multi_x.drop(columns = [\"income_binary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb(torch.Tensor(scaler.transform(np.array(x.values).reshape(1, -1)))).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(multi_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptor.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptor[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when\n",
    "explanation = tabularLore.explain(x)\n",
    "# then\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for premise in explanation[\"rule\"]['premises']:\n",
    "    print(premise[\"attr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/lcorbucci/synth_xai/artifacts/adult/comparison_explanation/lore/lore_1.pkl\", \"rb\") as f:\n",
    "    shap_explanation = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explanation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explanation[1][\"rule\"][\"premises\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lore Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.dataset import TabularDataset\n",
    "\n",
    "# from lore_sa.lorem import LOREM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    def __init__(self, model: torch.nn.Module, scaler) -> None:\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> torch.Tensor:\n",
    "\n",
    "        x = self.scaler.transform(x)\n",
    "\n",
    "        predictions = []\n",
    "        for sample in x:\n",
    "            sample = torch.Tensor(sample)\n",
    "            predictions.append(self.model(sample).argmax().item())\n",
    "        return np.array(predictions)\n",
    "        # print(x)\n",
    "        # x = torch.Tensor(x)\n",
    "        # print(x)\n",
    "        # return np.array([self.model(sample).argmax().item() for sample in x])\n",
    "    \n",
    "\n",
    "seed = 112\n",
    "current_script_path = Path(\"./\").resolve()\n",
    "print(current_script_path)\n",
    "x_train, _, x_test, _, _, _, train_df, test_data = prepare_letter(sweep=False, seed=seed)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "feature_names = list(train_df.drop(columns=[\"letter\"]).columns)\n",
    "\n",
    "bb_path = \"../../../artifacts/letter/bb/letter_BB.pth\"\n",
    "bb = load_bb(bb_path)\n",
    "\n",
    "bb.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "train_df_tmp = copy.copy(train_df)\n",
    "train_df_tmp = train_df_tmp.drop(columns=[\"letter\"])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(train_df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(model=bb, scaler=scaler)\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"letter\"] = [int(x) for x in train_df[\"letter\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"letter\"] = train_df[\"letter\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TabularDataset.from_dict(train_df, class_name = \"letter\")\n",
    "dataset.df.dropna(inplace = True)\n",
    "dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptor.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.bbox import sklearn_classifier_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = sklearn_classifier_bbox.sklearnBBox(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.lore import TabularRandomGeneratorLore\n",
    "\n",
    "tabularLore = TabularRandomGeneratorLore(bbox, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row = 10\n",
    "x = dataset.df.iloc[num_row][:-1] # we exclude the target feature\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 100):\n",
    "#     x = dataset.df.iloc[i][:-1] # we exclude the target feature\n",
    "#     print(bb(torch.Tensor(scaler.transform(np.array(x.values).reshape(1, -1)))).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100\n",
    "multi_x = dataset.df.iloc[:num_row] # we exclude the target feature\n",
    "multi_x = multi_x.drop(columns = [\"letter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb(torch.Tensor(scaler.transform(np.array(x.values).reshape(1, -1)))).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(multi_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptor.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.descriptor[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when\n",
    "explanation = tabularLore.explain(x)\n",
    "# then\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for premise in explanation[\"rule\"]['premises']:\n",
    "    print(premise[\"attr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/lcorbucci/synth_xai/artifacts/adult/comparison_explanation/lore/lore_1.pkl\", \"rb\") as f:\n",
    "    shap_explanation = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explanation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Shap explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/lcorbucci/synth_xai/artifacts/adult/comparison_explanation/shap/shap_1.pkl\", \"rb\") as f:\n",
    "    shap_explanation = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explanation[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aix_model:\n",
    "    def __init__(self, model: torch.nn.Module) -> None:\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> torch.Tensor:\n",
    "        x = torch.Tensor(x)\n",
    "        return self.model(x).argmax(dim=1)\n",
    "\n",
    "    def predict_proba(self, x: np.ndarray) -> np.ndarray:\n",
    "        # since the activation function of the last layer is LogSoftmax\n",
    "        # we need to apply the exponential to the output of the model\n",
    "        # cast x to be a Tensor\n",
    "        x = torch.Tensor(x)\n",
    "        return torch.nn.functional.softmax(self.model(x)).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 112\n",
    "current_script_path = Path(\"./\").resolve()\n",
    "\n",
    "x_train, _, x_test, _, _, _, train_df, test_data = prepare_adult(sweep=False, seed=seed, current_path=current_script_path)\n",
    "scaler = MinMaxScaler()\n",
    "_ = scaler.fit_transform(x_train)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "feature_names = list(train_df.drop(columns=[\"income_binary\"]).columns)\n",
    "\n",
    "# Identify categorical features (before one-hot encoding)\n",
    "categorical_features = [\n",
    "    train_df.columns.get_loc(col)\n",
    "    for col in [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "    if col in train_df.columns\n",
    "]\n",
    "\n",
    "# Define the class names\n",
    "class_names = [\"<=50K\", \">50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = []\n",
    "feature_names = [\"age\",\"household_position\",\"household_size\",\"prev_residence_place\",\"citizenship\",\"country_birth\",\"edu_level\",\"economic_status\", \"cur_eco_activity\",\"Marital_status\",\"sex_binary\", \"occupation_binary\"]\n",
    "class_names = [\"<=50K\", \">50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(\n",
    "    x_train,  # Unscaled training data\n",
    "    mode=\"classification\",\n",
    "    feature_names=feature_names,\n",
    "    categorical_features=categorical_features,\n",
    "    class_names=class_names,\n",
    "    discretize_continuous=True,  # Discretize continuous features for better interpretability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_path = \"../../../artifacts/adult/bb/adult_BB.pth\"\n",
    "bb = load_bb(bb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.to(\"cpu\")\n",
    "model = aix_model(model=bb)\n",
    "# model = bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(x: np.ndarray) -> np.ndarray:\n",
    "        model.model.to(\"cpu\")\n",
    "        prediction = model.predict_proba(x)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46324448])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate explanation\n",
    "sample_idx = 51  # Choose any index from x_test\n",
    "sample = np.array(x_test[sample_idx])\n",
    "explanation = explainer.explain_instance(sample, predict_fn, num_features=10)\n",
    "explanation.as_list()\n",
    "explanation.local_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.60 < edu_level <= 1.00', -0.4550781982046549),\n",
       " ('sex_binary <= 0.00', 0.27073169153941756),\n",
       " ('age <= 0.27', 0.09339496829436184),\n",
       " ('prev_residence_place <= 0.00', 0.07893404544743478),\n",
       " ('country_birth <= 0.00', -0.05718273197171243),\n",
       " ('citizenship <= 0.00', -0.041228587391163876),\n",
       " ('Marital_status <= 0.00', 0.028421249345087073),\n",
       " ('0.11 < household_position <= 0.19', -0.01062319708006088),\n",
       " ('cur_eco_activity > 0.93', -0.009474145521056973),\n",
       " ('household_size <= 0.07', 0.004768681924036221)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "feature_names = [feature for feature, weight in explanation.as_list()]\n",
    "clean_features = [re.sub(r'[<>]=?|\\d+(\\.\\d+)?', '', feature).strip() for feature in feature_names]\n",
    "\n",
    "clean_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"/home/lcorbucci/synth_xai/artifacts/adult/comparison_explanation/lime/\"\n",
    "file_name = \"lime_1.pkl\"\n",
    "store_path = Path(store_path) / file_name\n",
    "with Path(store_path).open(\"rb\") as f:\n",
    "    explanation = dill.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation[45][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"/home/lcorbucci/synth_xai/artifacts/adult/comparison_explanation/lime/\"\n",
    "file_name = \"lime_2.pkl\"\n",
    "store_path = Path(store_path) / file_name\n",
    "with Path(store_path).open(\"rb\") as f:\n",
    "    explanation = dill.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation[45][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"/home/lcorbucci/synth_xai/artifacts/adult/explanations/\"\n",
    "file_name = \"dt_tvae_100000_2500_1.pkl\"\n",
    "store_path = Path(store_path) / file_name\n",
    "with Path(store_path).open(\"rb\") as f:\n",
    "    explanation = dill.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import shap\n",
    "\n",
    "from synth_xai.bb_architectures import MultiClassModel, SimpleModel\n",
    "from synth_xai.explanations.explanation_utils import load_bb\n",
    "from synth_xai.utils import prepare_adult\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_representative_samples(dataset,samples_per_class=5):\n",
    "#     '''this function is used to get the representative samples of the dataset. but in a different way.\n",
    "#     First there is a clustering of all the data. The best k is retrieved using the elbow method.\n",
    "#     Then, for each cluster we order the samples by the distance from the centroid in 4 bins.\n",
    "#     Then we take some samples of each bin as the representative sample of the cluster, (in order to have the label)'''\n",
    "#     # print(dataset.tensors[1].shape,\"shape of the labels\")\n",
    "#     num_classes = 2\n",
    "#     representative_samples = np.empty((0,dataset.shape[1]))\n",
    "#     labels = np.empty((0,num_classes))\n",
    "#     # create the kmeans object\n",
    "#     inertias = []\n",
    "#     max_K = min(12,dataset.shape[0])\n",
    "#     for i in range(2,max_K):\n",
    "#         kmeans = KMeans(n_clusters=i, random_state=42,n_init=3)\n",
    "#         kmeans.fit(dataset)\n",
    "#         # get the centroids of the clusters\n",
    "#         inertias.append(kmeans.inertia_)\n",
    "\n",
    "#     # coupute the best k\n",
    "#     inertias_diff = np.diff(inertias)\n",
    "#     # find the last highest difference\n",
    "#     k = inertias_diff[::-1].argmax()\n",
    "#     k = len(inertias) - k\n",
    "#     print(\"best k:\",k)\n",
    "#     # plot the elbow plot\n",
    "\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42,n_init=3)\n",
    "#     kmeans.fit(dataset)\n",
    "#     # get the centroids of the clusters\n",
    "#     centroids = kmeans.cluster_centers_\n",
    "#     # get the samples that are mapped to each cluster\n",
    "#     labels = kmeans.labels_\n",
    "#     representative_labels = np.empty((0,num_classes))\n",
    "#     for cluster in range(k):\n",
    "#         # get the samples of the cluster\n",
    "#         cluster_samples = dataset[labels==cluster,:]\n",
    "#         # get the distances of the samples from the centroid\n",
    "#         distances = np.linalg.norm(cluster_samples-centroids[cluster],axis=1)\n",
    "#         # order the samples in 4 bins using again KMeans, but on the distances\n",
    "#         kkkk = min(4,cluster_samples.shape[0])\n",
    "#         kmeans = KMeans(n_clusters=kkkk, random_state=42,n_init=3)\n",
    "#         kmeans.fit(distances.reshape(-1,1))\n",
    "#         ordered_samples = cluster_samples[np.argsort(kmeans.labels_)]\n",
    "\n",
    "        \n",
    "#         # divide the samples in 4 bins\n",
    "#         bins = [ordered_samples[kmeans.labels_==i] for i in range(4)]\n",
    "#         # plot the number of samples in each bin\n",
    "    \n",
    "#         original_labels_of_cluster = dataset[labels==cluster,:]\n",
    "#         print(original_labels_of_cluster)\n",
    "        \n",
    "                                            \n",
    "#         # take the first sample of each bin, and its relative label\n",
    "#         for d_i in range(4):\n",
    "#             # if there are no samples in the bin, skip it\n",
    "#             if len(bins[d_i]) == 0:\n",
    "#                 continue\n",
    "#             # choose a random sample from the bin\n",
    "#             rnd_sample = 0#np.random.choice(bins[d_i])\n",
    "#             representative_samples = np.vstack((representative_samples,bins[d_i][rnd_sample]))\n",
    "#             print(original_labels_of_cluster[rnd_sample,:])\n",
    "#             print(representative_labels)\n",
    "#             representative_labels = np.vstack([representative_labels,\n",
    "#                                                 original_labels_of_cluster[rnd_sample,:]])\n",
    "                                    \n",
    "#     print(representative_samples.shape,representative_labels.shape)\n",
    "#     return representative_samples,representative_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats, labs = get_representative_samples(x_train[0:40], samples_per_class=30//num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lcorbucci/synth_xai/src/synth_xai/comparison\n"
     ]
    }
   ],
   "source": [
    "class aix_model:\n",
    "    def __init__(self, model: torch.nn.Module) -> None:\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> torch.Tensor:\n",
    "        x = torch.Tensor(x)\n",
    "        return self.model(x).argmax(dim=1)\n",
    "\n",
    "    def predict_proba(self, x: np.ndarray) -> np.ndarray:\n",
    "        # since the activation function of the last layer is LogSoftmax\n",
    "        # we need to apply the exponential to the output of the model\n",
    "        x = torch.Tensor(x)\n",
    "        return torch.nn.functional.softmax(self.model(x), dim=1).detach().numpy()\n",
    "\n",
    "seed = 112\n",
    "current_script_path = Path(\"./\").resolve()\n",
    "print(current_script_path)\n",
    "x_train, _, x_test, _, _, _, train_df, test_data = prepare_dutch(sweep=False, seed=seed, current_path=current_script_path)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "feature_names = list(train_df.drop(columns=[\"occupation_binary\"]).columns)\n",
    "\n",
    "bb_path = \"../../../artifacts/dutch/bb/dutch_BB.pth\"\n",
    "bb = load_bb(bb_path)\n",
    "\n",
    "bb.to(\"cpu\")\n",
    "model = aix_model(model=bb)\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SHAP explainer\n",
    "# explainer = shap.Explainer(model.predict_proba, x_test)\n",
    "explainer = shap.KernelExplainer(model.predict_proba, data=shap.kmeans(x_test, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "age                        7\n",
      "household_position      1131\n",
      "household_size           112\n",
      "prev_residence_place       1\n",
      "citizenship                1\n",
      "country_birth              1\n",
      "edu_level                  5\n",
      "economic_status          111\n",
      "cur_eco_activity         138\n",
      "Marital_status             1\n",
      "sex_binary                 0\n",
      "occupation_binary          0\n",
      "Name: 2229, dtype: int64\n",
      "\n",
      "Top 3 most similar samples:\n",
      "\n",
      "Similar sample 1 (similarity: 1.0000):\n",
      "age                        7\n",
      "household_position      1131\n",
      "household_size           112\n",
      "prev_residence_place       1\n",
      "citizenship                1\n",
      "country_birth              1\n",
      "edu_level                  5\n",
      "economic_status          111\n",
      "cur_eco_activity         138\n",
      "Marital_status             1\n",
      "sex_binary                 0\n",
      "occupation_binary          0\n",
      "Name: 51705, dtype: int64\n",
      "\n",
      "Similar sample 2 (similarity: 1.0000):\n",
      "age                        7\n",
      "household_position      1131\n",
      "household_size           112\n",
      "prev_residence_place       1\n",
      "citizenship                1\n",
      "country_birth              1\n",
      "edu_level                  5\n",
      "economic_status          111\n",
      "cur_eco_activity         138\n",
      "Marital_status             1\n",
      "sex_binary                 0\n",
      "occupation_binary          0\n",
      "Name: 14594, dtype: int64\n",
      "\n",
      "Similar sample 3 (similarity: 1.0000):\n",
      "age                        7\n",
      "household_position      1131\n",
      "household_size           112\n",
      "prev_residence_place       1\n",
      "citizenship                1\n",
      "country_birth              1\n",
      "edu_level                  5\n",
      "economic_status          111\n",
      "cur_eco_activity         138\n",
      "Marital_status             1\n",
      "sex_binary                 0\n",
      "occupation_binary          1\n",
      "Name: 21989, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "sample = test_data.iloc[sample_idx]\n",
    "\n",
    "# Get the feature values of the sample (excluding target column)\n",
    "sample_features = sample.drop('occupation_binary').values.reshape(1, -1)\n",
    "\n",
    "# Get all other samples (excluding target column)\n",
    "other_samples = test_data.drop(columns=['occupation_binary'])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarities = cosine_similarity(sample_features, other_samples)\n",
    "\n",
    "# Get indices of top 3 most similar samples (excluding itself if present)\n",
    "top_similar_indices = np.argsort(-similarities[0])[:4]  # Get 4 in case the sample itself is included\n",
    "top_similar_indices = [idx for idx in top_similar_indices if test_data.index[idx] != sample.name][:3]\n",
    "\n",
    "# Display the top 3 most similar samples\n",
    "print(\"Sample:\")\n",
    "print(sample)\n",
    "print(\"\\nTop 3 most similar samples:\")\n",
    "for i, idx in enumerate(top_similar_indices):\n",
    "    similar_sample = test_data.iloc[idx]\n",
    "    similarity = similarities[0][idx]\n",
    "    print(f\"\\nSimilar sample {i+1} (similarity: {similarity:.4f}):\")\n",
    "    print(similar_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        7\n",
       "household_position      1131\n",
       "household_size           112\n",
       "prev_residence_place       1\n",
       "citizenship                1\n",
       "country_birth              1\n",
       "edu_level                  5\n",
       "economic_status          111\n",
       "cur_eco_activity         138\n",
       "Marital_status             1\n",
       "sex_binary                 0\n",
       "occupation_binary          0\n",
       "Name: 2229, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.63it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate explanation\n",
    " # Choose any index from x_test\n",
    "sample = x_test[sample_idx:sample_idx+1]  # Keep sample as a batch\n",
    "shap_values = explainer(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27272727, 0.19090909, 0.06666667, 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.96428571, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = model.predict(sample)\n",
    "predicted_class = predicted_class.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86464745, 0.13535258]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.plots.watearfall(shap_values[0,:,predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00782544,  0.00422991, -0.00202537, -0.00261117,  0.        ,\n",
       "        0.00448547,  0.47456479, -0.01099995,  0.04268176, -0.01348994,\n",
       "       -0.09987371])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values.values[0,:,predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = shap_values[0,:,predicted_class].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00782544,  0.00422991, -0.00202537, -0.00261117,  0.        ,\n",
       "        0.00448547,  0.47456479, -0.01099995,  0.04268176, -0.01348994,\n",
       "       -0.09987371])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(feature_names, feature_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple example with Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from __future__ import print_function\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital Status\",\"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\"Hours per week\", \"Country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('./adult.data', delimiter=', ', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[:,14]\n",
    "le= sklearn.preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels = le.transform(labels)\n",
    "class_names = le.classes_\n",
    "data = data[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [1,3,5, 6,7,8,9,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_names = {}\n",
    "for feature in categorical_features:\n",
    "    le = sklearn.preprocessing.LabelEncoder()\n",
    "    le.fit(data[:, feature])\n",
    "    data[:, feature] = le.transform(data[:, feature])\n",
    "    categorical_names[feature] = le.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "encoder = ColumnTransformer(\n",
    "\ttransformers=[\n",
    "\t\t('cat', sklearn.preprocessing.OneHotEncoder(), categorical_features)\n",
    "\t],\n",
    "\tremainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.fit(data)\n",
    "encoded_train = encoder.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "gbtree = xgboost.XGBClassifier(n_estimators=300, max_depth=5)\n",
    "gbtree.fit(encoded_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(labels_test, gbtree.predict(encoder.transform(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: gbtree.predict_proba(encoder.transform(x)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(train ,feature_names = feature_names,class_names=class_names,\n",
    "                                                   categorical_features=categorical_features, \n",
    "                                                   categorical_names=categorical_names, kernel_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "i = 1\n",
    "exp = explainer.explain_instance(test[i], predict_fn, num_features=5)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.local_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
