{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acaebf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from types import FrameType\n",
    "from typing import Any\n",
    "\n",
    "import dill\n",
    "import multiprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from synth_xai.explanations.explanation_utils import (\n",
    "    evaluate_bb,\n",
    "    find_top_closest_rows,\n",
    "    get_test_data,\n",
    "    is_explainer_supported,\n",
    "    label_synthetic_data,\n",
    "    load_bb,\n",
    "    load_synthetic_data,\n",
    "    make_predictions,\n",
    "    prepare_neighbours,\n",
    "    setup_wandb,\n",
    "    transform_input_data,\n",
    ")\n",
    "from loguru import logger\n",
    "from multiprocess import Pool\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from synth_xai.utils import (\n",
    "    prepare_adult,\n",
    "    prepare_breast_cancer,\n",
    "    prepare_covertype,\n",
    "    prepare_diabetes,\n",
    "    prepare_dutch,\n",
    "    prepare_house16,\n",
    "    prepare_letter,\n",
    "    prepare_pima,\n",
    "    prepare_shuttle,\n",
    ")\n",
    "\n",
    "from synth_xai.bb_architectures import MultiClassModel, SimpleModel\n",
    "from synth_xai.explanations.explainer_model import ExplainerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c783b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"../artifacts/dutch/explanations/logistic_tvae_100000_2500_1.pkl\"\n",
    "with Path(store_path).open(\"rb\") as f:\n",
    "    logistic_explanations = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bc301f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sex_binary: coefficient=-92.56249402713907, value=0',\n",
       "  'edu_level: coefficient=-48.93522179546129, value=3',\n",
       "  'citizenship: coefficient=17.84497095300212, value=1',\n",
       "  'Marital_status: coefficient=-17.20016342561355, value=2',\n",
       "  'age: coefficient=-14.963362112520459, value=7',\n",
       "  'prev_residence_place: coefficient=-14.65777058946945, value=1',\n",
       "  'country_birth: coefficient=8.140114941576273, value=1',\n",
       "  'economic_status: coefficient=6.505416047619395, value=111',\n",
       "  'household_size: coefficient=-5.8086956664258915, value=113',\n",
       "  'cur_eco_activity: coefficient=-2.5422864730615435, value=138',\n",
       "  'household_position: coefficient=0.5672736517644529, value=1122'],\n",
       " 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_explanations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac5dec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"../artifacts/dutch/explanations/dt_tvae_100000_2500_1.pkl\"\n",
    "with Path(store_path).open(\"rb\") as f:\n",
    "    dt_explanations = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a5532ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['(edu_level = 3) <= 4.5',\n",
       "  '(sex_binary = 0) <= 0.5',\n",
       "  '(prev_residence_place = 1) <= 1.5',\n",
       "  'Leaf node 3 reached, prediction: 1'],\n",
       " 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_explanations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b76f25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"../artifacts/dutch/explanations/knn_tvae_100000_2500_1.pkl\"\n",
    "with Path(store_path).open(\"rb\") as f:\n",
    "    knn_explanations = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3792d013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['KNN prediction: 1',\n",
       "  'Nearest neighbors (index, distance, label):',\n",
       "  'Index: 134, distance: 0.0000, label: 1, sample: [   7 1122  113    1    1    1    3  111  138    2    0]',\n",
       "  'Index: 25, distance: 0.0000, label: 1, sample: [   7 1122  113    1    1    1    3  111  138    2    0]',\n",
       "  'Index: 40, distance: 0.0000, label: 1, sample: [   7 1122  113    1    1    1    3  111  138    2    0]'],\n",
       " 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_explanations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57fdef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"./synth_xai/explanations/\")\n",
    "_, _, _, _, _, _, train_df, test_data = prepare_dutch(\n",
    "                sweep=False, seed=42, current_path=file_path\n",
    "            )\n",
    "\n",
    "bb = load_bb(\"/home/lcorbucci/synth_xai/artifacts/dutch/bb/dutch_BB.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cfa4c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-26 12:32:27.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msynth_xai.explanations.explanation_utils\u001b[0m:\u001b[36mevaluate_bb\u001b[0m:\u001b[36m217\u001b[0m - \u001b[1mAccuracy: 0.832092022509103 - F1: 0.8321849875595729\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "x, y, scaler = transform_input_data(train_data=train_df, test_data=test_data, outcome_variable=\"occupation_binary\")\n",
    "outputs = evaluate_bb(x, y, bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef34574",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(test_data)\n",
    "for index in range(num_samples):\n",
    "    sample = test_data.iloc[[index]]\n",
    "    x_sample = torch.tensor([x[index]])\n",
    "    y_sample = torch.tensor([y[index]])\n",
    "    sample_pred_bb = make_predictions(x_sample, y_sample, bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "664f681a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': {18273: 7},\n",
       " 'household_position': {18273: 1122},\n",
       " 'household_size': {18273: 113},\n",
       " 'prev_residence_place': {18273: 1},\n",
       " 'citizenship': {18273: 1},\n",
       " 'country_birth': {18273: 1},\n",
       " 'edu_level': {18273: 3},\n",
       " 'economic_status': {18273: 111},\n",
       " 'cur_eco_activity': {18273: 138},\n",
       " 'Marital_status': {18273: 2},\n",
       " 'sex_binary': {18273: 0},\n",
       " 'occupation_binary': {18273: 1}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[[0]].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef57b5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': {781: 11},\n",
       " 'household_position': {781: 1121},\n",
       " 'household_size': {781: 112},\n",
       " 'prev_residence_place': {781: 1},\n",
       " 'citizenship': {781: 1},\n",
       " 'country_birth': {781: 1},\n",
       " 'edu_level': {781: 3},\n",
       " 'economic_status': {781: 111},\n",
       " 'cur_eco_activity': {781: 135},\n",
       " 'Marital_status': {781: 2},\n",
       " 'sex_binary': {781: 0},\n",
       " 'occupation_binary': {781: 1}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[[134]].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5ca3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b026c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5a9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5666861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example code of a simple RNN, GRU, LSTM on the MNIST dataset.\n",
    "\n",
    "Programmed by Aladdin Persson <aladdin.persson at hotmail dot com>\n",
    "*    2020-05-09 Initial coding\n",
    "*    2022-12-16 Updated with more detailed comments, docstrings to functions, and checked code still functions as intended.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn  # All neural network modules\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment by creating mini batches etc.\n",
    "from tqdm import tqdm  # For a nice progress bar!\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "sequence_length = 28\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Recurrent neural network with GRU (many-to-one)\n",
    "class RNN_GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN_GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Recurrent neural network with LSTM (many-to-one)\n",
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(\n",
    "            x, (h0, c0)\n",
    "        )  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Load Data\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize network (try out just using simple RNN, or GRU, and then compare with LSTM)\n",
    "model = RNN_LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device).squeeze(1)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent update step/adam step\n",
    "        optimizer.step()\n",
    "\n",
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Set model to eval\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device).squeeze(1)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    # Toggle model back to train\n",
    "    model.train()\n",
    "    return num_correct / num_samples\n",
    "\n",
    "\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
